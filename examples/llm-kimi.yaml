apiVersion: inference.devopstoolkit.ai/v1alpha1
kind: LLMInference
metadata:
  name: kimi
spec:
  model: moonshotai/Kimi-K2.5
  gpu: 8
  ingressHost: kimi.127.0.0.1.nip.io
  providerConfigName: inference-large
  targetNamespace: production
