# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## What This Project Is

A Crossplane Configuration (`dot-inference`) that provides a minimal API for deploying LLM inference workloads on Kubernetes using vLLM Production Stack. Users specify `model`, `gpu`, `ingressHost`, and `providerConfigName` — the composition function derives everything else (CPU, memory, tensor parallelism, probes, env vars).

## Commands

All commands require `devbox shell` first. Tests output should be redirected to `./tmp/` for review.

```bash
task cluster-create          # KinD + Crossplane + providers + functions + VLLMRuntime CRD
task cluster-destroy         # Tear down KinD cluster
task package-generate        # Embed python/composition.py into package/composition.yaml
task package-apply           # Apply XRD + Composition to cluster
task package-generate-apply  # Both in sequence
task test-e2e                # package-generate-apply + chainsaw test
task test                    # Full cycle: cluster-create → test → cluster-destroy
task package-publish         # Build xpkg + push to Upbound Marketplace (needs UP_TOKEN, UP_ACCOUNT, VERSION)
```

Run tests with output capture: `task test-e2e 2>&1 | tee ./tmp/test-output.txt`

## Architecture

```
LLMInference XR (namespace-scoped, Crossplane v2 — no claims)
    → function-python executes inline python/composition.py
        → generates Object (VLLMRuntime CR) + Object (Ingress)
            → provider-kubernetes applies Objects to target cluster via ProviderConfig
                → vLLM Production Stack operator reconciles VLLMRuntime
```

### Key Directories

- **`package/`** — Crossplane Configuration: `crossplane.yaml` (metadata + deps), `definition.yaml` (XRD), `composition.yaml` (generated, gitignored)
- **`python/`** — Source of truth for composition logic: `composition.py` (function code) + `composition.yaml.tmpl` (Composition template with `__PYTHON_SCRIPT__` placeholder)
- **`providers/`** — Provider/function install manifests for local dev cluster (provider-kubernetes, function-python, incluster ProviderConfig)
- **`tests/`** — Chainsaw e2e tests: `definition/` (XRD schema assert) and `llm-inference/` (XR → Object → VLLMRuntime + Ingress assert)
- **`examples/`** — Phase 1 plain manifests (`vllm-*.yaml`) and Phase 2 XR examples (`llm-*.yaml`)

### Generation Flow

`python/composition.py` is the source of truth. Never edit `package/composition.yaml` directly — it's generated by `task package-generate` which embeds the Python code (indented 8 spaces) into `python/composition.yaml.tmpl` at the `__PYTHON_SCRIPT__` marker.

### Composed Resources

The composition function generates `kubernetes.crossplane.io/v1alpha2` Object resources. Each Object wraps either a `VLLMRuntime` CR or an `Ingress`, with `providerConfigRef` pointing to the user-specified ProviderConfig (target cluster).

## Critical Details

- **Image**: Must use `lmcache/vllm-openai:v0.3.13` — the vLLM Production Stack operator expects `/opt/venv/bin/vllm` entrypoint which only exists in the lmcache fork
- **Service port**: Operator creates service on port 80 (not 8000). Ingress routes to port 80
- **Crossplane v2**: Namespace-scoped XRs directly — no Claims. API version `apiextensions.crossplane.io/v2` for XRD
- **function-python**: Generic Python engine (`xpkg.crossplane.io/crossplane-contrib/function-python`). Python code runs inline, not as a custom function image. Limited to stdlib + `crossplane.function` SDK
- **GPU heuristics**: 1 GPU → 2 CPU/8Gi, 8 GPU → 16 CPU/64Gi. Tensor parallelism = GPU count when > 1. `VLLM_WORKER_MULTIPROC_METHOD=spawn` for multi-GPU
- **Test cluster**: KinD with VLLMRuntime CRD installed (no operator). Tests verify resource creation, not actual vLLM deployment
- **ProviderConfig**: `incluster` (InjectedIdentity) for testing. Examples use `gpu-cluster` for remote GPU clusters
